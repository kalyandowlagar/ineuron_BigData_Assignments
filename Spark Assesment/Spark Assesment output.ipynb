{"cells":[{"cell_type":"code","source":["vals = [\"Randy\",\"John sena\",\"Rock\"]\nrdd=sc.parallelize(vals)\nrdd2=rdd.map(lambda x : \"Hey \" + x)\nR=rdd2.collect()\nprint(R) # map is to apply only specific function to each element of rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2db905a2-ae9e-42a9-a9e9-738f7c8b5ed3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['Hey Randy', 'Hey John sena', 'Hey Rock']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['Hey Randy', 'Hey John sena', 'Hey Rock']\n"]}}],"execution_count":0},{"cell_type":"code","source":["vals = [\"Randy,35\",\"John sena,24\",\"Rock,48\",\"Rima Desai,42\",\"Priya mallay,18\",\"kali,38\"]\nrdd=sc.parallelize(vals)\nlisted=rdd.filter(lambda x:x if len(x.split(\" \"))==2 else None)\nm=listed.collect()\nprint(m)\nprint(listed.first())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"700df243-ce4b-4c26-9ff2-6495e3d90081"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['John sena,24', 'Rima Desai,42', 'Priya mallay,18']\nJohn sena,24\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['John sena,24', 'Rima Desai,42', 'Priya mallay,18']\nJohn sena,24\n"]}}],"execution_count":0},{"cell_type":"code","source":["vals = ['121','457','224522','111','272','1001','245542','487']\nrdd=sc.parallelize(vals)\nrdd2=rdd.filter(lambda x : x if x[::-1] == x else None)\nR=rdd2.collect()\nprint(R)\nprint(rdd2.first())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c200fd5-f6eb-489b-abb9-9163dba20816"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['121', '111', '272', '1001', '245542']\n121\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['121', '111', '272', '1001', '245542']\n121\n"]}}],"execution_count":0},{"cell_type":"code","source":["randy = [4,8,75,12,95,16,35,24,19,30,44]\nrdd=sc.parallelize(randy)\ndiv_2=rdd.reduce(lambda x,y :y if (x+y)%4 ==0 else x)#reduce takes two arguments as input parameters \nprint(div_2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99cc92bf-0430-4882-98b7-b20a6db04bd3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"16\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["16\n"]}}],"execution_count":0},{"cell_type":"code","source":["randy=[\"Hello Welcome to the world\",\"have fun tonight\"]\nrdd=sc.parallelize(randy)\nrdd1=rdd.flatMap(lambda x:x.split(\" \"))\nrdd2=rdd1.filter(lambda x:x if len(x)%2==0 else None)\nrdd3=rdd2.collect()\nprint(rdd3) # filter and reduce is not same as reduce is obtained from two or more partition but filter is just from one to one partition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"256129a8-8745-496d-a0a2-6f7a81625e1f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['to', 'have']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['to', 'have']\n"]}}],"execution_count":0},{"cell_type":"code","source":["def reformat(partitionData):\n    updatedData = []\n    for row in partitionData:\n        name=row[0]+\",\"+row[1]\n        bonus=row[2]*10/100\n        newsal=row[2]+bonus\n        updatedData.append([name,newsal])\n    return iter(updatedData)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf07db2d-5029-4dcc-97de-ab0ac354ea15"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["empsalarylist =[(\"ravi\",\"sharma\",24000),('kavya','tanneru',150000),(\"Emma\",\"jackson\",20000)]\nrdd=sc.parallelize(empsalarylist,3)\nrdd2 =rdd.mapPartitions(reformat)\nprint(rdd2.glom().collect())\nprint(rdd2.collect())# perform function at each partition level"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e578f3c5-dc7c-4ec0-9194-3fb83645be39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[[['ravi,sharma', 26400.0]], [['kavya,tanneru', 165000.0]], [['Emma,jackson', 22000.0]]]\n[['ravi,sharma', 26400.0], ['kavya,tanneru', 165000.0], ['Emma,jackson', 22000.0]]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[[['ravi,sharma', 26400.0]], [['kavya,tanneru', 165000.0]], [['Emma,jackson', 22000.0]]]\n[['ravi,sharma', 26400.0], ['kavya,tanneru', 165000.0], ['Emma,jackson', 22000.0]]\n"]}}],"execution_count":0},{"cell_type":"code","source":["empsalarylist =[24000,25800,150000,20000,12000,120800]\nrdd=sc.parallelize(empsalarylist,3)\ndef f(iterator): yield sum(iterator)\nrdd2 =rdd.mapPartitions(f)\nprint(rdd2.collect())\nprint(rdd2.take(2)) #take to obtain two values of result"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48f8e008-7d57-4a12-8dd4-731d0c7ab7ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[49800, 170000, 132800]\n[49800, 170000]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[49800, 170000, 132800]\n[49800, 170000]\n"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([1, 2, 3, 4,9,5,8,7],6)\ndef f(splitIndex, iterator): yield 'index: '+str(splitIndex)+\" values: \"+ str(sum(list(iterator)))\nrd=rdd.mapPartitionsWithIndex(f)\nprint(rd.collect())\nprint(rd.take(1)) #mapPartitionWithIndex is to obtain function with index value on each partition of RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6ccd121-904d-487b-9851-babd6dc2ac48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['index: 0 values: 1', 'index: 1 values: 2', 'index: 2 values: 7', 'index: 3 values: 9', 'index: 4 values: 5', 'index: 5 values: 15']\n['index: 0 values: 1']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['index: 0 values: 1', 'index: 1 values: 2', 'index: 2 values: 7', 'index: 3 values: 9', 'index: 4 values: 5', 'index: 5 values: 15']\n['index: 0 values: 1']\n"]}}],"execution_count":0},{"cell_type":"code","source":["parallel = sc.parallelize(range(1,10),3)\ndef show(index, iterator): yield 'index: '+str(index)+\" values: \"+ str(list(iterator))#yield is used as its a generator to hold iterator values\nrd = parallel.mapPartitionsWithIndex(show)\nprint(rd.count())\nprint(rd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a78d6716-a5e1-49f2-8c6b-4fb26d5c8c50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"3\n['index: 0 values: [1, 2, 3]', 'index: 1 values: [4, 5, 6]', 'index: 2 values: [7, 8, 9]']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["3\n['index: 0 values: [1, 2, 3]', 'index: 1 values: [4, 5, 6]', 'index: 2 values: [7, 8, 9]']\n"]}}],"execution_count":0},{"cell_type":"code","source":["parallel = sc.parallelize([1,3,4,5,23,24,53,41,56,62,94,41,24,82,62,82,95,2,3,0],4)\nprint(parallel.sample(True,0.5).collect()) # what is the significance of parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a2bc16e-3c6f-49f6-9bb4-37d3ba6e9eea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[1, 3, 4, 4, 5, 23, 23, 24, 24, 82, 82, 2, 2]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[1, 3, 4, 4, 5, 23, 23, 24, 24, 82, 82, 2, 2]\n"]}}],"execution_count":0},{"cell_type":"code","source":["#with duplicates as withreplacement parameter is false,\n#.4 is fractional size of rdd data, \n#seed is default none its number generator\nprint(parallel.sample(False,.4,5).collect()) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5531272-601f-4205-8d5d-b052e0e819c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[3, 4, 5, 53, 56, 94, 24, 62, 82]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[3, 4, 5, 53, 56, 94, 24, 62, 82]\n"]}}],"execution_count":0},{"cell_type":"code","source":["one = sc.parallelize([1,5,7,9,11,13,19])\ntwo = sc.parallelize([1,2,4,8,6])\nprint(one.union(two).collect())#gives duplicates "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d218dc3-5ee5-4ea3-b727-9313fd40d151"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[1, 5, 7, 9, 11, 13, 19, 1, 2, 4, 8, 6]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[1, 5, 7, 9, 11, 13, 19, 1, 2, 4, 8, 6]\n"]}}],"execution_count":0},{"cell_type":"code","source":["one = sc.parallelize([1,5,7,9,11,13,19,20])\ntwo = sc.parallelize([1,2,4,8,6,20])\nprint(one.intersection(two).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b926c257-c47b-47fe-adbf-d2288985c913"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[1, 20]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[1, 20]\n"]}}],"execution_count":0},{"cell_type":"code","source":["one = sc.parallelize([1,5,7,9,11,13,19,20])\ntwo = sc.parallelize([1,2,4,8,6,20])\nprint(one.union(two).distinct().collect())#distinct removes duplicates"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e170465f-3bc3-459d-8340-11be3ac630f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[1, 2, 19, 20, 4, 5, 6, 7, 8, 9, 11, 13]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[1, 2, 19, 20, 4, 5, 6, 7, 8, 9, 11, 13]\n"]}}],"execution_count":0},{"cell_type":"code","source":["x=(('10th std',\"Rahul\"),('9th std',\"Suresh\"),('10th std',\"mohan\"),('10th std',\"Ruthvik\"),('9th std',\"naresh\"),('9th std',\"Priya\"))\nrdd=sc.parallelize(x)\ny=rdd.groupByKey().map(lambda x : (x[0],list(x[1]))).collect()\nprint(y)\nprint(rdd.groupByKey().map(lambda x : {x[0]:list(x[1])}).collect())#can be coverted to list tuple or dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90202bbb-952f-46f6-813e-83898660e059"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('10th std', ['Rahul', 'mohan', 'Ruthvik']), ('9th std', ['Suresh', 'naresh', 'Priya'])]\n[{'10th std': ['Rahul', 'mohan', 'Ruthvik']}, {'9th std': ['Suresh', 'naresh', 'Priya']}]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('10th std', ['Rahul', 'mohan', 'Ruthvik']), ('9th std', ['Suresh', 'naresh', 'Priya'])]\n[{'10th std': ['Rahul', 'mohan', 'Ruthvik']}, {'9th std': ['Suresh', 'naresh', 'Priya']}]\n"]}}],"execution_count":0},{"cell_type":"code","source":["x=[(\"Rahul\",1),(\"Suresh\",1),(\"mohan\",1),(\"Ruthvik\",1),(\"naresh\",1),(\"Priya\",1),(\"Rahul\",1),(\"Suresh\",1),(\"mohan\",1),(\"Ruthvik\",1),(\"naresh\",1),(\"Priya\",1),(\"Ruthvik\",1),(\"naresh\",1),(\"Priya\",1),(\"Rahul\",1)]\nrdd=sc.parallelize(x)\nz=rdd.reduceByKey(lambda x,y : x+y)#reduce always takes two arguments as input\nprint(z.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12649b63-974d-4a02-9cb1-39247efcce66"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('naresh', 3), ('Priya', 3), ('Rahul', 3), ('Ruthvik', 3), ('Suresh', 2), ('mohan', 2)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('naresh', 3), ('Priya', 3), ('Rahul', 3), ('Ruthvik', 3), ('Suresh', 2), ('mohan', 2)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["listed= [('AADEN', 18),('AADEN', 11),('AADEN', 10),('AALIYAH', 50),('AALIYAH', 44),('Mahesh', 11),('Ramesh', 10),('suresh', 50),('Tarun', 44)]\nlated=sc.parallelize(listed)\nprint(lated.map(lambda n:(n[0],n[1])).sortByKey().collect())\nprint(lated.map(lambda n:(n[0],n[1])).sortByKey(False).collect())\nprint(lated.map(lambda n:(n[1],n[0])).sortByKey().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9ad0240-626b-466e-b927-93df5b8ce845"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('AADEN', 18), ('AADEN', 11), ('AADEN', 10), ('AALIYAH', 50), ('AALIYAH', 44), ('Mahesh', 11), ('Ramesh', 10), ('Tarun', 44), ('suresh', 50)]\n[('suresh', 50), ('Tarun', 44), ('Ramesh', 10), ('Mahesh', 11), ('AALIYAH', 50), ('AALIYAH', 44), ('AADEN', 18), ('AADEN', 11), ('AADEN', 10)]\n[(10, 'AADEN'), (10, 'Ramesh'), (11, 'AADEN'), (11, 'Mahesh'), (18, 'AADEN'), (44, 'AALIYAH'), (44, 'Tarun'), (50, 'AALIYAH'), (50, 'suresh')]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('AADEN', 18), ('AADEN', 11), ('AADEN', 10), ('AALIYAH', 50), ('AALIYAH', 44), ('Mahesh', 11), ('Ramesh', 10), ('Tarun', 44), ('suresh', 50)]\n[('suresh', 50), ('Tarun', 44), ('Ramesh', 10), ('Mahesh', 11), ('AALIYAH', 50), ('AALIYAH', 44), ('AADEN', 18), ('AADEN', 11), ('AADEN', 10)]\n[(10, 'AADEN'), (10, 'Ramesh'), (11, 'AADEN'), (11, 'Mahesh'), (18, 'AADEN'), (44, 'AALIYAH'), (44, 'Tarun'), (50, 'AALIYAH'), (50, 'suresh')]\n"]}}],"execution_count":0},{"cell_type":"code","source":["names1 = sc.parallelize([\"ape\", \"yieepy\", \"apple\"])#join is unclear to me\nnames2 = sc.parallelize([\"apple\", \"beauty\", \"beatles\"])\nprint(names2.join(names1).collect())\nprint(names2.union(names1).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4fe9f80-571f-4c43-971a-52ff8b8d5f2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('a', ('p', 'p')), ('a', ('p', 'p'))]\n['apple', 'beauty', 'beatles', 'ape', 'yieepy', 'apple']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('a', ('p', 'p')), ('a', ('p', 'p'))]\n['apple', 'beauty', 'beatles', 'ape', 'yieepy', 'apple']\n"]}}],"execution_count":0},{"cell_type":"code","source":["num1= sc.parallelize([2,4,1,5,1,5,3,6,7,9])\nnum2= sc.parallelize([4,5,7,8,2,7,2,8,2,1])\nprint(num1.union(num2).collect())\nprint(num1.join(num2).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ca28f1c-cd40-4d76-91eb-1a253d7c8561"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[2, 4, 1, 5, 1, 5, 3, 6, 7, 9, 4, 5, 7, 8, 2, 7, 2, 8, 2, 1]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[2, 4, 1, 5, 1, 5, 3, 6, 7, 9, 4, 5, 7, 8, 2, 7, 2, 8, 2, 1]\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2077936163396408>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mnum2\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    964\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 966\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    967\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    968\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 164.0 failed 1 times, most recent failure: Lost task 4.0 in stage 164.0 (TID 1906) (ip-10-172-236-55.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 752, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 744, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2312, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:693)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:837)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:646)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:822)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:572)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2264)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:364)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2582)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor391.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 752, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 744, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2312, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:693)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:837)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:646)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:822)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:572)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2264)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:364)\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 164.0 failed 1 times, most recent failure: Lost task 4.0 in stage 164.0 (TID 1906) (ip-10-172-236-55.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable'. Full traceback below:","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2077936163396408>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mnum2\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0msc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallelize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m7\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    964\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 966\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    967\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    968\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 164.0 failed 1 times, most recent failure: Lost task 4.0 in stage 164.0 (TID 1906) (ip-10-172-236-55.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 752, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 744, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2312, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:693)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:837)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:646)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:822)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:572)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2264)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:364)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2582)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor391.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: 'int' object is not subscriptable'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 752, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 744, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 264, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/databricks/spark/python/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2312, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:693)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:855)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:837)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:646)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:442)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:822)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:572)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2264)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:364)\n"]}}],"execution_count":0},{"cell_type":"code","source":["x = sc.parallelize([(\"a\", 1), (\"b\", 4)])#More or less its similar to groupbykey but however we can put together two rdd's\ny = sc.parallelize([(\"a\", 2)])\n[(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9c77b13-1823-4a71-a4ff-2d95a4690fb5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[62]: [('a', ([1], [2])), ('b', ([4], []))]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[62]: [('a', ([1], [2])), ('b', ([4], []))]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([1,2])#every element in rdd is grouped with rdd1 element\nrdd1 = sc.parallelize([9,8,6])\nsorted(rdd.cartesian(rdd1).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf9ced46-60ab-43ba-855f-708264a1e09b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[63]: [(1, 6), (1, 8), (1, 9), (2, 6), (2, 8), (2, 9)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[63]: [(1, 6), (1, 8), (1, 9), (2, 6), (2, 8), (2, 9)]"]}}],"execution_count":0},{"cell_type":"code","source":["sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()#pipe is used to pass command on each partition of RDD and send to StdOut of any environmrnt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef127451-0b1d-46fa-8a89-f37b590b27f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[64]: ['1', '2', '', '3']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[64]: ['1', '2', '', '3']"]}}],"execution_count":0},{"cell_type":"code","source":["l = sc.parallelize([1,3,4,5,23,24,53,41,56,62,3,0],6)#coalesce is prossible on rdd's only and it dont shuffle all elements but transfers element to neareest rdd\nm=l.coalesce(4)\nprint(l.getNumPartitions())\nprint(m.getNumPartitions())\nm.saveAsTextFile(\"/dbfs/coalesce2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef60da68-5f54-4a06-adeb-589ec58a14c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"6\n4\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["6\n4\n"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\n\nls -ltr /dbfs/coa*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5044b578-50e6-457c-8693-fc503e94b6bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ls: cannot access '/dbfs/coa*': No such file or directory\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ls: cannot access '/dbfs/coa*': No such file or directory\n"]}}],"execution_count":0},{"cell_type":"code","source":["p = sc.parallelize([1,3,4,5,23,24,53,41,3,0],7)#repartition is prossible on rdd's only and it shuffles all elements and create equal sized rdd\nq=l.repartition(3)\nprint(p.getNumPartitions())\nprint(q.getNumPartitions())\nq.saveAsTextFile(\"c:/tmp/repartition2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea0feb84-787c-4d05-ba73-ab36d05440ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"7\n3\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["7\n3\n"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)],3)\nrdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\nprint(rdd.glom().collect())\nprint(rdd2.glom().collect())\nprint(rdd2.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"860d0178-6b3a-4115-93be-e0c7418e9d7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[[(0, 5), (3, 8)], [(2, 6), (0, 8)], [(3, 8), (1, 3)]]\n[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n[(0, 5), (0, 8), (2, 6), (1, 3), (3, 8), (3, 8)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[[(0, 5), (3, 8)], [(2, 6), (0, 8)], [(3, 8), (1, 3)]]\n[[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n[(0, 5), (0, 8), (2, 6), (1, 3), (3, 8), (3, 8)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["a=sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7])\na.takeOrdered(6)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ce7c361-090f-4569-afee-69147464c8f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[103]: [1, 2, 3, 4, 5, 6]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[103]: [1, 2, 3, 4, 5, 6]"]}}],"execution_count":0},{"cell_type":"code","source":["b=sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2)\nb.takeOrdered(6, key=lambda x: -x)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b94c55c4-0565-4d8e-b762-7c8fa464a10c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[101]: [10, 9, 7, 6, 5, 4]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[101]: [10, 9, 7, 6, 5, 4]"]}}],"execution_count":0},{"cell_type":"code","source":["b=sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2)\nb.takeOrdered(6,key=lambda x: x in range(1,6,3))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ee89999-acc8-4995-9c84-366c55a172ac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[109]: [10, 2, 9, 3, 5, 6]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[109]: [10, 2, 9, 3, 5, 6]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1),(\"a\", 1), (\"b\", 1), (\"a\", 1),(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nsorted(rdd.countByKey().items())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"993b812c-c1bc-4116-97ea-13f5414a8a27"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[110]: [('a', 6), ('b', 3)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[110]: [('a', 6), ('b', 3)]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize(range(0, 10))\nm=rdd.foreach(lambda x: x + 1)\ntype(m)\nrdd.foreach(lambda x: print(x))#foreach function returns no value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"644d5fdb-7ec5-4c48-9acf-145e04052cf2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fddc4f6-1f00-40f7-ae69-f5e6ad07e888"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"mynewnote","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1091347376968296}},"nbformat":4,"nbformat_minor":0}
