{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cbfaad8-42d7-4146-b6ca-f153b1507f92"}}},{"cell_type":"markdown","source":["# CSE Coding Assignment\n## Instructions\n\n- Please answer all questions\n- You can use any language you wish (e.g. Python, Scala, SQL...)\n- Several Markdown cells require completion. Please edit the Markdown cells to include your answer.\n- Your final notebook should compile without errors when you click \"Run All\"\n\n**Please do not publish questions. This is a confidential assignment.**\n\n### Creating a Cluster\n\nYou will need to create a Databricks Cluster. More information on this process is available here: https://docs.databricks.com/user-guide/clusters/create.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79e6aa93-4d81-4cfb-82f4-cb24e5c5b33c"}}},{"cell_type":"markdown","source":["## Getting Started\n\n**REQUIRED:** Run the following cells exactly as written to retrieve the necessary Coding Assignment Data Sets from Amazon S3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af916113-b24d-4b0c-8543-ccefb6094c48"}}},{"cell_type":"code","source":["%sh curl --remote-name-all 'https://files.training.databricks.com/assessments/cse-take-home/{covertype,kafka,treecover,u.data,u.item}.csv'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4622ef0-0774-40b6-a490-8464619c3f4e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/databricks/driver/covertype.csv\", \"dbfs:/FileStore/tmp/covertype.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/kafka.csv\", \"dbfs:/FileStore/tmp/kafka.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/treecover.csv\", \"dbfs:/FileStore/tmp/treecover.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.data.csv\", \"dbfs:/FileStore/tmp/u.data.csv\")\ndbutils.fs.cp(\"file:/databricks/driver/u.item.csv\", \"dbfs:/FileStore/tmp/u.item.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e050ea2d-e539-4efa-aa18-813102bbe3e5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Part 1: Reading and Parsing Data\n\n### Question 1:  Code Challenge - Load a CSV\n\n- Load the CSV file at `dbfs:/FileStore/tmp/nl/treecover.csv` into a DataFrame.\n- Use Apache Spark to read in the data, assigned to the variable `treeCoverDF`.\n- Your method to get the CSV file into Databricks isn't graded. We are only concerned with how you use Spark to parse and load the actual data. \n- Please use the `inferSchema` option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6017a5eb-8c81-460e-902d-2b69588a7bd5"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58e3f180-5eed-4f10-838f-f96408365dcc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 2:  Code Challenge - Print the Schema\n\nUse Apache Spark to display the Schema of the `treeCoverDF` Dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7507f053-7a27-4422-aa1a-86dd75037f1b"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d817e77-2d16-4f02-80be-e80af133ca82"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 3:  Code Challenge - Rows & Columns\n\nUse Apache Spark to display the number of rows and columns in the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e44ffca2-82f1-43ea-b873-912674165b67"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84ca332b-64b2-4608-98ad-95dc7a0925a1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Part 2: Analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14c4e7ed-31f5-4ec2-b8a0-95b505985c35"}}},{"cell_type":"markdown","source":["### Question 4:  Code Challenge - Summary Statistics for a Feature\n\nUse Apache Spark to answer these questions about the `treeCoverDF` DataFrame:\n- What is the range - minimum and maximum - of values for the feature `elevation`?\n- What are the mean and standard deviation of the feature `elevation`?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cecb0ee4-834c-484c-9a8d-a1763cb22109"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a58613-9c3c-46fe-8a43-7cbcc8df2875"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Answer #4:\n\n- Min `elevation`: `YOUR ANSWER HERE`\n- Max `elevation`: `YOUR ANSWER HERE`\n- Mean `elevation`: `YOUR ANSWER HERE`\n- Standard Deviation of `elevation`: `YOUR ANSWER HERE`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eacc2888-9f7d-4d36-9e08-d0eac0808c4e"}}},{"cell_type":"markdown","source":["### Question 5:  Code Challenge - Record Count\n\nUse Apache Spark to answer the following question:\n- How many entries in the dataset have an `elevation` greater than or equal to 2749.32 meters **AND** a `Cover_Type` of 1 or 2?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e94fccb6-891e-4092-9d46-8b32a9265b16"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ac85423-80db-45d8-b580-609d3a6ef929"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 6: Code Challenge - Compute a Percentage\n\nUse Apache Spark to answer the following question:\n- What percentage of entries with `Cover_Type` 1 or 2 have an `elevation` at or above 2749.32 meters?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51ff1431-1305-46fc-a59a-7b2378770d4a"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ec8a16b-d4e7-4498-85cf-abe310618b2d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 7: Code Challenge - Visualize Feature Distribution\n\nUse any [visualization tool available in the Databricks Runtime](https://docs.databricks.com/user-guide/visualizations/index.html) to generate the following visualization:\n\n- a bar chart that helps visualize the distribution of different Wilderness Areas in our dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed1a9f4-703b-4332-b31f-bcea9ad80b54"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77a28563-31ab-4e56-8730-4d9a1ede84d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 8: Code Challenge - Visualize Average Elevation by Cover Type \n\nUse any [visualization tool available in the Databricks Runtime](https://docs.databricks.com/user-guide/visualizations/index.html) to generate the following visualization:\n\n- a bar chart showing the average elevation of each cover type with string labels for cover type\n\n**NOTE: you will need to match the integer values in the column `treeCoverDF.Cover_Type` to the string values in `dbfs:/FileStore/tmp/nl/covertype.csv` to retrieve the Cover Type Labels. It is recommended to use an Apache Spark join.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"821447ca-4ff7-448a-9cad-10691d55bef6"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"992c86ce-502a-4f58-967f-023813bfbd9c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Part 3: Data Ingestion, Cleansing, and Transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c9c08c2-633c-48f4-88ce-64fef0946c02"}}},{"cell_type":"markdown","source":["## Instructions \n\nThis is a multi-step, data pipeline question in which you need to achieve a few objectives to build a successful job.\n\n### Data Sets\n\n#### `u.data.csv`\n\n- The full u data set, 100000 ratings by 943 users on 1682 items. \n- Each user has rated at least 20 movies.  \n- Users and items are numbered consecutively from 1. \n- The data is randomly ordered. \n- This is a tab separated file consisting of four columns: \n   - user id \n   - movie id \n   - rating \n   - date (unix seconds since 1/1/1970 UTC)\n\n#### Desired schema\n\n- `user_id INTEGER`\n- `movie_id INTEGER`\n- `rating INTEGER`\n- `date DATE `\n\n#### `u.item.csv`\n\n- This is a `|` separated file consisting of six columns:\n   - movie id\n   - movie title\n   - release date\n   - video release date\n   - IMDb URL\n   - genre\n- movie ids in this file match movie ids in `u.data`.\n\n#### Desired schema\n\n- `movie_id INTEGER`\n- `movie_title STRING`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7324d228-43d3-4eb8-8624-35fe5a11be26"}}},{"cell_type":"markdown","source":["### Question 9:  Code Challenge - Load DataFrames\n\nUse Apache Spark to perform the following:\n1. define the correct schemas for each Data Set to be imported as described above  \n   **note:** \n      - for `u.data.csv`, `date` *must* be stored using `DateType` with the format `yyyy-MM-dd`\n      - you may need to ingest `timestamp` data using `IntegerType`\n      - be sure to drop unneccesary columns for `u.item.csv`\n1. import the two files as DataFrames names `uDataDF` and `uItemDF` using the schemas you defined and these paths:\n   - `dbfs:/FileStore/tmp/u.data.csv`\n   - `dbfs:/FileStore/tmp/u.item.csv`\n1. order the `uDataDF` DataFrame by the `date` column\n\n**NOTE:** Please display the DataFrames, `uDataDF` and `uItemDF` after loading."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83b2d516-a0ea-4665-a654-fb2da2e26b18"}}},{"cell_type":"markdown","source":["#### `uDataDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a7e6125-08ad-4431-85bc-63a94c70e3e0"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb26c967-2f8f-4f44-88e4-889760214a11"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### `uItemDF`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46675f81-dc38-4b36-8621-bb5a1956c54b"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"829d780b-9c33-4bfa-a46e-a4538b4cc5c9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 10:  Code Challenge - Perform a Join\n\nUse Apache Spark to do the following:\n- join `uDataDF` and `uItemDf` on `movie_id` as a new DataFrame called `uMovieDF`  \n   **note:** make sure you do not create duplicate `movie_id` columns\n   \n**NOTE:** Please display the DataFrame `uMovieDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"686d417e-2180-43a7-8785-66abb3a9fcf9"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7c405db-dd36-46ca-9ebc-6e811d53c3ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 11:  Code Challenge - Perform an Aggregation\n\nUse Apache Spark to do the following:\n1. create an aggregate DataFrame, `aggDF` by\n  1. extracting the year from the `date` (of the review)\n  1. getting the average rating of each film per year as a column named `average_rating`\n  1. ordering descending by year and average rating\n1. write the resulting dataframe to a table named \"movie_by_year_average_rating\" in the Default database  \n   **note:** use `mode(overwrite)` \n\n#### Desired Schema\nThe schema of you resulting DataFrame should be:\n- `year INTEGER`\n- `movie_title STRING`\n- `average_rating DOUBLE`\n\n**NOTE:** Please display the DataFrame `aggDF`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f033344-61ea-4ea7-a51d-ebaf22be8e62"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"565e6948-92f1-4c25-a215-78c14faa8c8e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Part 4: Fun with JSON\n\nJSON values are typically passed by message brokers such as Kafka or Kinesis in a string encoding. When consumed by a Spark Structured Streaming application, this json must be converted into a nested object in order to be used.\n\nBelow is a list of json strings that represents how data might be passed from a message broker.\n\n**Note:** Make sure to run the cell below to retrieve the sample data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c84485-ab71-4cb8-85bc-244494d2cb4d"}}},{"cell_type":"code","source":["%python\n\n\nsampleJson = [\n ('{\"user\":100, \"ips\" : [\"191.168.192.101\", \"191.168.192.103\", \"191.168.192.96\", \"191.168.192.99\"]}',), \n ('{\"user\":101, \"ips\" : [\"191.168.192.102\", \"191.168.192.105\", \"191.168.192.103\", \"191.168.192.107\"]}',), \n ('{\"user\":102, \"ips\" : [\"191.168.192.105\", \"191.168.192.101\", \"191.168.192.105\", \"191.168.192.107\"]}',), \n ('{\"user\":103, \"ips\" : [\"191.168.192.96\", \"191.168.192.100\", \"191.168.192.107\", \"191.168.192.101\"]}',), \n ('{\"user\":104, \"ips\" : [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.102\", \"191.168.192.99\"]}',), \n ('{\"user\":105, \"ips\" : [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.100\", \"191.168.192.96\"]}',), \n]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba6bf8b7-4348-4b81-bc5f-d3b3c940db55"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 12:  Code Challenge - Count the IPs\n\nUse any coding techniques known to you to parse this list of JSON strings to answer the following question:\n- how many occurrences of each IP address are in this list?\n\n#### Desired Output\nYour results should be this:\n\n\n| ip | count |\n|:-:|:-:|\n| `191.168.192.96` | `3` |\n| `191.168.192.99` | `6` |\n| `191.168.192.100` | `2` |\n| `191.168.192.101` | `3` |\n| `191.168.192.102` | `2` |\n| `191.168.192.103` | `2` |\n| `191.168.192.105` | `3` |\n| `191.168.192.107` | `3` |\n\n**NOTE:** The order of your results is not important."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e34335a-ad2d-4ace-b01c-6144a67afad2"}}},{"cell_type":"code","source":["# YOUR CODE HERE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"356e7fd8-d527-4e25-ac9f-2e006942b048"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Part 5: The Databricks API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57b8af12-a907-4c89-9ce4-1e6a9fecbcd9"}}},{"cell_type":"markdown","source":["### Question 13: Conceptual Question - the Databricks API\n\nIn 4-5 sentences, please explain what the Databricks API is used for at a high-level."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2fcae3b-2d26-40d8-812e-83df5a2f402e"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cd0bbb2-0bb1-49f3-a002-2eef2a7096a7"}}},{"cell_type":"markdown","source":["### Question 14: Conceptual Question - Explain an API Call\n\nIn 4-5 sentences, please explain what this API call. Be sure to discuss some key attributes about the cluster.\n\n```\n$ curl -n -X POST -H 'Content-Type: application/json'                      \\\n  -d '{                                                                     \\\n  \"cluster_name\": \"high-concurrency-cluster\",                               \\\n  \"spark_version\": \"4.2.x-scala2.11\",                                       \\\n  \"node_type_id\": \"i3.xlarge\",                                              \\\n  \"spark_conf\":{                                                            \\\n        \"spark.databricks.cluster.profile\":\"serverless\",                    \\\n        \"spark.databricks.repl.allowedLanguages\":\"sql,python,r\"             \\\n     },                                                                     \\\n     \"aws_attributes\":{                                                     \\\n        \"zone_id\":\"us-west-2c\",                                             \\\n        \"first_on_demand\":1,                                                \\\n        \"availability\":\"SPOT_WITH_FALLBACK\",                                \\\n        \"spot_bid_price_percent\":100                                        \\\n     },                                                                     \\\n   \"custom_tags\":{                                                          \\\n        \"ResourceClass\":\"Serverless\"                                        \\\n     },                                                                     \\\n       \"autoscale\":{                                                        \\\n        \"min_workers\":1,                                                    \\\n        \"max_workers\":2                                                     \\\n     },                                                                     \\\n  \"autotermination_minutes\":10                                              \\\n}' https://dogfood.staging.cloud.databricks.com/api/2.0/clusters/create '\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0bfb77a-4f99-43c4-993a-7b5acb48db49"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24c8ad63-ed58-4704-91ef-95f2e99c0a60"}}},{"cell_type":"markdown","source":["## Part 6: Security"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"326ef8fd-bce0-46a2-b17e-65dbd1e687e7"}}},{"cell_type":"markdown","source":["### Question 15: Conceptual Question - Security on Databricks\n\nUsing the Databricks Documentation, what would you recommend to a Databricks and AWS customer for **securely** storing and accessing their data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74d8a02c-08a3-4a58-9ec2-f493c4f22025"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b9a2086-89b1-469a-9719-acdbc0803339"}}},{"cell_type":"markdown","source":["# This is the end of the official test. Bonus below!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fefd4c92-ca46-4415-a4a4-e20b6743f52a"}}},{"cell_type":"markdown","source":["## Part 7: Bonus: Data Science & Machine Learning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8291308-a452-4735-9e42-70f17d69fe09"}}},{"cell_type":"markdown","source":["### Question 16: Conceptual Question - A Skewed Feature\n\nOne of these lines is the *mean* of this feature. The other is the *median*. Which of these lines is the **mean** - the red line or the black line?\n\n<img width=400px src=https://www.evernote.com/l/AAEycL6CQ0hLi5V5pIo91Ko-Pfk2i0AnGyMB/image.png>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51f7c7bb-9692-446a-89d5-53901b4bd0d8"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26e458c4-5872-4a00-ad3c-529894bde6eb"}}},{"cell_type":"markdown","source":["### Question 17: Conceptual Question - Exploratory Data Analysis\n\nThe plots below show the distribution of home selling prices differentiated by a few categorical features. Based on these plots, **which of these categorical features** - Property Type, Exterior Quality, or Month Sold - would you expect to be most associated with Price? **Why**?\n\n<img width=1600px src=https://www.evernote.com/l/AAHulkcc20hHSJV6D1udKiwSDCN0S6oV_5YB/image.png>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19932477-22f0-41ee-8b57-bd90fafcd515"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f16f4fe6-828f-47ab-b3fc-3fd43132cac8"}}},{"cell_type":"markdown","source":["### Question 18: Conceptual Question - Analyze Model Performance\n\nConsider the following results for a decision tree model against training and testing data sets:\n\n`decision tree regression - train r2 score: 0.9944`  \n`decision tree regression - test r2 score:  0.3119`\n\n\nWhat is your assessment of this model?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e729420-5523-42c6-8688-ea0033cbbea9"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"351bd437-bc37-4ff5-8167-b72bf3500418"}}},{"cell_type":"markdown","source":["### Question 19: Conceptual Question - Model Selection\n\nA series of models has been built using the same training data, but each with a subset of features.\n\nConsider the following results for a series of logistic regression models and then answer this question:\n\n- Which model would you choose and why?\n- What other things would you want to look at and why?\n\n| model number | feature subset | logistic regression test accuracy|\n|:-:|:-:|:-:|\n| 1| feat_1 |\t 0.631|\n| 2| feat_2 |\t 0.552|\n| 3| feat_3 |\t 0.868|\n| 4| feat_4 |\t 0.868|\n| 5| feat_1, feat_2 |\t 0.657|\n| 6| feat_1, feat_3 |\t 0.947|\n| 7| feat_1, feat_4 |\t 0.921|\n| 8| feat_2, feat_3 |\t 0.947|\n| 9| feat_2, feat_4 |\t 0.973|\n| 10| feat_3, feat_4 |\t 0.947|\n| 11| feat_1, feat_2, feat_3 |\t 0.947|\n| 12| feat_1, feat_2, feat_4 |\t 0.947|\n| 13| feat_1, feat_3, feat_4 |\t 0.947|\n| 14| feat_2, feat_3, feat_4 |\t 0.973|\n| 15| feat_1, feat_2, feat_3, feat_4 |\t 0.973|"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf31f496-31a5-4278-9dc5-2162056b2a9c"}}},{"cell_type":"markdown","source":["### Answer:\n\n`EDIT THIS MARKDOWN CELL WITH YOUR REPLY`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13d0afaa-509b-4003-82a0-0e1b2622f1a8"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2019 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cdc6c4f-2425-4c2b-a991-6b4c42172d77"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"cse-take-home-assignment","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2458874312093997}},"nbformat":4,"nbformat_minor":0}
